{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620a7164-74d3-4bce-8073-31d03292dae4",
   "metadata": {},
   "source": [
    "># Q1. What is Gradient Boosting Regression?\n",
    "## Gradient Boosting Regression (GBR) is a type of boosting algorithm that builds a strong predictive model by iteratively combining multiple weak models. It is a machine learning technique used for regression and classification problems, which involves the iterative training of weak models, such as decision trees, to improve the accuracy of the overall model.\n",
    "\n",
    "## In GBR, each weak model is trained to correct the errors of the previous model in the sequence. The algorithm starts with an initial weak model, which makes predictions based on a set of input features. Then, the errors of the initial model are calculated, and a new model is trained to predict the errors made by the initial model. The new model is then added to the initial model to create a better model. This process is repeated iteratively until the desired level of accuracy is achieved or until a stopping criterion is met.\n",
    "\n",
    "## During the training process, the algorithm adjusts the weights of the samples in the training set to focus on the errors made by the previous model. The final model is an ensemble of all the weak models in the sequence, each contributing a small amount to the overall prediction. The final model is capable of making accurate predictions on new data, even when the relationships between the input features and the target variable are complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d095fad4-89d2-441a-8700-4bb814bab376",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b782934c-41e4-4340-b0f0-6a6233264219",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e721123-04da-46a3-9441-060810ce9557",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11c9e6f0-a18f-4adf-9251-57aebf5258d4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0ff725a-a093-46fd-85a4-607dccdb9477",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ece30ecd-3f38-4a50-b111-c2c456129f78",
   "metadata": {},
   "source": [
    "># Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8402f93-d9ea-4aaa-87ce-fdf14ebfbe26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # initialize the residuals with the target values\n",
    "        residuals = y.copy()\n",
    "\n",
    "        # loop over the number of estimators\n",
    "        for i in range(self.n_estimators):\n",
    "            # fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # predict the residuals for the current tree\n",
    "            residuals_pred = tree.predict(X)\n",
    "\n",
    "            # update the residuals with the predictions\n",
    "            residuals -= self.learning_rate * residuals_pred\n",
    "\n",
    "            # add the current tree to the list of trees\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # initialize the predictions with zeros\n",
    "        y_pred = np.zeros(X.shape[0])\n",
    "\n",
    "        # loop over the trees and add the predictions\n",
    "        for tree in self.trees:\n",
    "            y_pred += self.learning_rate * tree.predict(X)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c072a1c8-bc7e-4246-a712-e9254d503536",
   "metadata": {},
   "source": [
    "## In this implementation, we define a `GradientBoostingRegressor` class that takes three hyperparameters as input: `n_estimators`, `learning_rate`, and `max_depth`. `n_estimators` specifies the number of decision trees to include in the ensemble, `learning_rate` controls the contribution of each tree to the final prediction, and `max_depth` specifies the maximum depth of each decision tree.\n",
    "\n",
    "## The `fit` method fits the gradient boosting regressor to the training data `X` and target values `y`. We start by initializing the residuals with the target values, and then loop over the number of estimators. In each iteration, we fit a decision tree to the residuals and predict the residuals for the current tree. We then update the residuals with the predictions and add the current tree to the list of trees.\n",
    "\n",
    "## The `predict` method takes an input matrix `X` and returns the predicted target values using the ensemble of decision trees.\n",
    "\n",
    "### To evaluate the performance of the gradient boosting regressor, we can use metrics such as mean squared error (MSE) and R-squared. Here's an example of how to use these metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f95200c1-f8f1-4ae8-9b1a-f40331eed4e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 622.8360400994117\n",
      "R-squared: 0.8237145944942637\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "# generate a random regression problem\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=1)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=4)\n",
    "\n",
    "# train the gradient boosting regressor\n",
    "gb = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)\n",
    "gb.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing set\n",
    "y_pred = gb.predict(X_test)\n",
    "\n",
    "# calculate mean squared error and R-squared\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"MSE:\", mse)\n",
    "print(\"R-squared:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d045c298-7498-4f54-8e63-745f19a9f93f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "353165cb-c1cb-4b2b-a3fc-7d56d8210d85",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba0f24bf-0d87-49ce-9473-64e7dd59d216",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f7d74a7-8813-48be-a95b-de20af125ac1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20c7dc1c-6c0f-4c84-aae4-b3653eda5b9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7e09649b-b758-4267-ae1f-232bf0bc47e0",
   "metadata": {},
   "source": [
    "># Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "48d22116-37d8-4348-ba3c-47db1ce4ad32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "becb9134-5718-4ce1-b00e-1498f33af3f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "817d255f-0dac-4fd4-9281-4aec0f700579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-16 {color: black;background-color: white;}#sk-container-id-16 pre{padding: 0;}#sk-container-id-16 div.sk-toggleable {background-color: white;}#sk-container-id-16 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-16 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-16 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-16 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-16 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-16 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-16 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-16 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-16 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-16 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-16 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-16 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-16 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-16 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-16 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-16 div.sk-item {position: relative;z-index: 1;}#sk-container-id-16 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-16 div.sk-item::before, #sk-container-id-16 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-16 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-16 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-16 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-16 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-16 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-16 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-16 div.sk-label-container {text-align: center;}#sk-container-id-16 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-16 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-16\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-32\" type=\"checkbox\" checked><label for=\"sk-estimator-id-32\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f7317ad1-35c0-49ca-b4c7-086f5cb76cec",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.654812760850612"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addfcdec-3d4c-4fb5-87ce-f638ef0d9905",
   "metadata": {},
   "source": [
    ">## Hypterparameter Tunning using RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1dc7ea3-34f5-4b6c-b556-d1726d42b9a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings as w\n",
    "w.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "046c4c53-adf6-453d-a4ce-d55e0ff2b86f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "parameters = {'criterion':['friedman_mse', 'squared_error'],\n",
    "            'loss':['squared_error','absolute_error','huber','quantile'],\n",
    "            'learning_rate':[0.001,0.01,0.01],\n",
    "            'n_estimators':[100,200],\n",
    "            'min_samples_split':[2,3,4,5],\n",
    "            'min_samples_leaf':[1,2,3,4,5],\n",
    "            'max_depth':[2,3,4,5],\n",
    "            'max_features':['auto', 'sqrt', 'log2']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "768c1edc-a63d-47c7-a5b5-5e4abc337770",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.8 s\n",
      "Wall time: 40.2 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {color: black;background-color: white;}#sk-container-id-10 pre{padding: 0;}#sk-container-id-10 div.sk-toggleable {background-color: white;}#sk-container-id-10 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-10 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-10 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-10 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-10 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-10 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-10 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-10 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-10 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-10 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-10 div.sk-item {position: relative;z-index: 1;}#sk-container-id-10 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-10 div.sk-item::before, #sk-container-id-10 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-10 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-10 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-10 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-10 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-10 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-10 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-10 div.sk-label-container {text-align: center;}#sk-container-id-10 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-10 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;,\n",
       "                                                      &#x27;squared_error&#x27;],\n",
       "                                        &#x27;learning_rate&#x27;: [0.001, 0.01, 0.01],\n",
       "                                        &#x27;loss&#x27;: [&#x27;squared_error&#x27;,\n",
       "                                                 &#x27;absolute_error&#x27;, &#x27;huber&#x27;,\n",
       "                                                 &#x27;quantile&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [2, 3, 4, 5],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;,\n",
       "                                                         &#x27;log2&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 3, 4, 5],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 3, 4, 5],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200]},\n",
       "                   scoring=&#x27;r2&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-24\" type=\"checkbox\" ><label for=\"sk-estimator-id-24\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "                   param_distributions={&#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;,\n",
       "                                                      &#x27;squared_error&#x27;],\n",
       "                                        &#x27;learning_rate&#x27;: [0.001, 0.01, 0.01],\n",
       "                                        &#x27;loss&#x27;: [&#x27;squared_error&#x27;,\n",
       "                                                 &#x27;absolute_error&#x27;, &#x27;huber&#x27;,\n",
       "                                                 &#x27;quantile&#x27;],\n",
       "                                        &#x27;max_depth&#x27;: [2, 3, 4, 5],\n",
       "                                        &#x27;max_features&#x27;: [&#x27;auto&#x27;, &#x27;sqrt&#x27;,\n",
       "                                                         &#x27;log2&#x27;],\n",
       "                                        &#x27;min_samples_leaf&#x27;: [1, 2, 3, 4, 5],\n",
       "                                        &#x27;min_samples_split&#x27;: [2, 3, 4, 5],\n",
       "                                        &#x27;n_estimators&#x27;: [100, 200]},\n",
       "                   scoring=&#x27;r2&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-25\" type=\"checkbox\" ><label for=\"sk-estimator-id-25\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "                   param_distributions={'criterion': ['friedman_mse',\n",
       "                                                      'squared_error'],\n",
       "                                        'learning_rate': [0.001, 0.01, 0.01],\n",
       "                                        'loss': ['squared_error',\n",
       "                                                 'absolute_error', 'huber',\n",
       "                                                 'quantile'],\n",
       "                                        'max_depth': [2, 3, 4, 5],\n",
       "                                        'max_features': ['auto', 'sqrt',\n",
       "                                                         'log2'],\n",
       "                                        'min_samples_leaf': [1, 2, 3, 4, 5],\n",
       "                                        'min_samples_split': [2, 3, 4, 5],\n",
       "                                        'n_estimators': [100, 200]},\n",
       "                   scoring='r2')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "random_model = RandomizedSearchCV(model,param_distributions=parameters,scoring='r2',cv=5,n_iter=10)\n",
    "random_model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "33fc158d-408a-480b-b6d6-95e401f4f31f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 200,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 4,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_depth': 4,\n",
       " 'loss': 'huber',\n",
       " 'learning_rate': 0.01,\n",
       " 'criterion': 'squared_error'}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d812ac43-b7db-4905-a4b4-3a46fd60d186",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7413262441016575"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954dc947-0681-4c9a-8fa3-dcde8c3a56fa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2be649d0-a051-4e37-a446-757c6d5d6b06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1755101e-5e4d-44d1-a713-6a9691f74ecb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07332851-3718-4df2-b480-675ee24a2f2e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa39ef79-0321-4df1-b7bf-db36eecd2963",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d60b1895-f04b-46e4-870b-023071d69c54",
   "metadata": {},
   "source": [
    "># Q4. What is a weak learner in Gradient Boosting?\n",
    "## In Gradient Boosting Regression, a weak learner is a decision tree model that performs only slightly better than random guessing. The goal of the Gradient Boosting algorithm is to iteratively combine these weak learners to create a strong learner that can accurately predict the target variable. The weak learner typically has a small depth and a small number of nodes, which reduces the risk of overfitting. In each iteration of the algorithm, the weak learner is trained on the residuals of the previous iteration. The residuals are the differences between the predicted and actual values of the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c1943a-7c39-450a-bdd5-852ef35dab54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d6dead4-f493-4ccc-860c-ae1613684aa3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebc906fd-be53-4cf4-a39d-7586cfef8a3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe0ce681-15d8-4902-989f-6215d3c3462f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "411e5933-3097-41bd-b475-7fd846274111",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ce9c735-335d-4755-bdfd-63ec3884a1ed",
   "metadata": {},
   "source": [
    "># Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "## The intuition behind the Gradient Boosting algorithm is to iteratively improve a model by focusing on the residuals (or errors) of the previous model. It works by adding new weak models (often decision trees) to the ensemble, with each model trained to correct the errors made by the previous models. \n",
    "\n",
    "## In the beginning, the Gradient Boosting algorithm trains the first model on the entire dataset, then the subsequent models are trained on the residuals of the previous model. The algorithm continues to iteratively minimize the residuals by adding new models to the ensemble until a stopping criterion is met, such as reaching a maximum number of models or achieving a certain level of performance. \n",
    "\n",
    "## The final model is a weighted sum of all the weak models, where the weights are determined by the performance of each weak model. The Gradient Boosting algorithm is known to be a powerful and flexible method for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8b9c79-d3a9-41eb-95dc-eb800172f257",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce9bb600-cb7e-4ed3-9dce-e7676b60a389",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "393d9df6-318c-4d89-94a8-8fe9f905c001",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e1e22b1-6311-4964-909a-a33a6a330102",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4933c004-a14b-4a91-bc02-e0488fd7eae6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "267684b9-97ca-4b3e-a102-9197c5b838fc",
   "metadata": {},
   "source": [
    "># Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "## Gradient Boosting algorithm builds an ensemble of weak learners in a stage-wise manner. It starts by fitting a single weak learner (usually a decision tree) to the training data and calculating the residuals of the predictions. The next weak learner is then fitted to these residuals, rather than the original target values, with the aim of reducing the error made by the previous learner. This process is repeated iteratively, with each subsequent learner fitted to the residuals of the previous ensemble of learners, until a predefined number of learners has been fitted or a convergence criterion has been met.\n",
    "\n",
    "## Each weak learner is fit to the gradient of the loss function with respect to the predictions of the previous ensemble of learners. This means that each subsequent learner is fitted to the negative gradient of the loss function with respect to the predictions made by the current ensemble of learners, which ensures that the next learner tries to correct the mistakes made by the previous learner.\n",
    "\n",
    "## The final ensemble of weak learners is then combined to make predictions on new data. The combination is typically a weighted sum of the predictions made by each weak learner, with the weights determined by the performance of each learner on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5d408-71af-4dc8-a84b-ce536aedfb1e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7097408b-6ccb-4e81-8d90-2f2e5cb297d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85656523-d381-410d-8fe0-a3473f1bd948",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9e8b6a9-fdf6-4648-99e0-1769ae0b49bd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2626b34-18f0-4991-adab-daad4191db31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b69f030a-ea0f-4a29-802b-86fe7dc98aa6",
   "metadata": {
    "tags": []
   },
   "source": [
    "># Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "## Here are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm:\n",
    "\n",
    "* ## 1. We start by assuming a function to approximate the target variable. This function is usually set to a constant value which is the mean of the target variable.\n",
    "* ## 2. We then calculate the errors by taking the difference between the actual target variable and the predicted values from step 1.\n",
    "* ## 3. We fit a new model, called a weak learner, to the errors. This model is typically a decision tree with a small depth.\n",
    "* ## 4. We add the predictions from this weak learner to our previous approximation, with a shrinkage parameter that controls the contribution of the weak learner. The new function becomes a better approximation of the target variable.\n",
    "* ## 5. We repeat steps 2-4 until a stopping criterion is met, such as a maximum number of iterations or a minimum improvement in performance.\n",
    "* ## 6. Finally, we obtain the final model by combining the weak learners with their corresponding shrinkage parameters, and use it to make predictions on new data.\n",
    "\n",
    "## By repeating this process, the algorithm learns to gradually improve the approximation of the target variable, using the residuals of the previous model as the new target variable for the next model. This approach allows the algorithm to build a highly accurate model, even with complex, non-linear relationships between the input and output variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
