{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cca11f9-3575-448d-a07e-323d3c5c423c",
   "metadata": {},
   "source": [
    "># Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "## Min-Max scaling is a data preprocessing technique used to scale features to a range of [0, 1]. It is also known as normalization and is generally performed during the data preprocessing step. Min-Max scaling is useful when you have a feature with a wide range of values, and you want to scale it to a range that is more manageable. For example, if you have a feature with values ranging from 0 to 1000, you can scale it to a range of [0, 1] using Min-Max scaling.\n",
    "### Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e9a1bd-f197-4edb-a1ea-a9e8c621f8a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.        ]\n",
      " [0.33333333 0.33333333]\n",
      " [0.66666667 0.66666667]\n",
      " [1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# Create a 2D array00\n",
    "data = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "\n",
    "# Create the scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the scaler to the data\n",
    "scaler.fit(data)\n",
    "\n",
    "# Transform the data\n",
    "scaled_data = scaler.transform(data)\n",
    "\n",
    "print(scaled_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d40ca7e-7451-4713-91c4-51de8fc4b813",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83dac4ce-702d-49ce-bb0f-89f2786d400a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab6a1414-9eb8-4244-8de7-44cc948dd73d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "782085c9-3904-4e24-bbdc-06dc1607f0ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "450a9345-a94b-4972-807c-7d0098c1ba32",
   "metadata": {},
   "source": [
    "# Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "## The Unit Vector technique is a feature scaling technique that is done considering the whole feature vector to be of unit length. This means that each component of the feature vector is divided by the Euclidean length of the vector (L2 Norm). The Unit Vector technique produces values of range [0,1]. This technique is quite useful when dealing with features with hard boundaries. For example, when dealing with image data, the colors can range from only 0 to 255.\n",
    "\n",
    "## The difference between the Unit Vector technique and Min-Max scaling is that Min-Max scaling scales the data to a fixed range of values, usually [0,1] or [-1,1], while the Unit Vector technique scales the data to a unit length.\n",
    "\n",
    "### Here’s an example to illustrate the application of the Unit Vector technique: Suppose we have a dataset with two features, x1 and x2. The values of x1 range from 0 to 100, while the values of x2 range from 0 to 1. we can apply the Unit Vector technique to scale the features. First, we calculate the Euclidean length of the feature vector, which is sqrt(x1^2 + x2^2). Then, we divide each component of the feature vector by the Euclidean length to get the scaled feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61331a3f-8da4-4354-bd27-73f2889c4bd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d186809-6cde-47d3-b350-d8215ec0b38e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec66b691-5f30-4ae7-a9ae-34bca1c27f47",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eebf74a2-0609-48e8-aa96-90c9323086a1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8607cbf1-c3a8-4100-81da-041332ace8a9",
   "metadata": {},
   "source": [
    "# Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "## Principal Component Analysis (PCA) is a technique used for dimensionality reduction. It is a statistical method that is used to reduce the number of variables in a dataset while retaining as much information as possible. PCA works by identifying the principal components of the data, which are the directions in which the data varies the most. These principal components are then used to create a new set of variables that are uncorrelated with each other.\n",
    "\n",
    "## PCA is used in a variety of applications, such as face recognition, image compression, and data visualization. Here’s an example to illustrate its application: Suppose we have a dataset with three features, x1, x2, and x3. we can apply PCA to reduce the dimensionality of the dataset. First, we calculate the covariance matrix of the dataset. Then, we calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components of the data, and the eigenvalues represent the amount of variance explained by each principal component. we can then select the top k eigenvectors with the highest eigenvalues to create a new set of k variables that are uncorrelated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa7e3df-385f-4787-bf56-c96504d4e6f0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c96a7e0-5b1e-41e2-9405-5a05c302e485",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b566294-d360-45a7-b039-3b52e98ed3fb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d66e47a-b6c5-4ddc-b09d-93b0030043c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7f19e30-89dd-4ded-b3dc-d4c1b9c212d1",
   "metadata": {},
   "source": [
    "# Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "## PCA is a feature extraction technique that is used to reduce the dimensionality of a dataset. It works by identifying the principal components of the data, which are the directions in which the data varies the most. These principal components are then used to create a new set of variables that are uncorrelated with each other.\n",
    "\n",
    "## Feature extraction is the process of creating new features from existing ones. PCA is one of the most commonly used feature extraction techniques. It is used to reduce the dimensionality of a dataset by identifying the most important features. PCA can be used for feature extraction by selecting the top k eigenvectors with the highest eigenvalues to create a new set of k variables that are uncorrelated with each other.\n",
    "\n",
    "### Here’s an example to illustrate this concept: Suppose we have a dataset with five features, x1, x2, x3, x4, and x5. we can apply PCA to reduce the dimensionality of the dataset. First, we calculate the covariance matrix of the dataset. Then, we calculate the eigenvectors and eigenvalues of the covariance matrix. The eigenvectors represent the principal components of the data, and the eigenvalues represent the amount of variance explained by each principal component. we can then select the top k eigenvectors with the highest eigenvalues to create a new set of k variables that are uncorrelated with each other. These new variables can be used as features in a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc716d3-e525-448d-91c9-81a5a16ad765",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba6bf869-586b-4091-9e73-b86d079aec84",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1391bf40-ba9a-43ae-a615-41ec70741802",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cf75246-f79c-4ad1-a206-a8d1ff87c8ed",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7992e46-374f-4052-8686-7960ff88f564",
   "metadata": {},
   "source": [
    "# Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "## Min-Max scaling is a technique used to scale the values of a dataset to a fixed range. It is used to normalize the data so that it falls within a specific range. In the case of a food delivery service recommendation system, Min-Max scaling can be used to preprocess the data to ensure that all the features are on the same scale.\n",
    "\n",
    "## To use Min-Max scaling, we first need to determine the minimum and maximum values of each feature in the dataset. Once we have these values, we can use the following formula to scale the values of each feature to a fixed range:\n",
    "\n",
    "## scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "## For example, suppose we have a dataset with three features: price, rating, and delivery time. The minimum and maximum values for each feature are as follows:\n",
    "\n",
    "### Price: 5 to 50\n",
    "### Rating: 1 to 5\n",
    "### Delivery time: 10 to 60\n",
    "## To scale the values of each feature to a fixed range of 0 to 1, we would use the following formula:\n",
    "\n",
    "## scaled_value = (value - min_value) / (max_value - min_value)\n",
    "\n",
    "### Scaled price = (price - 5) / (50 - 5)\n",
    "### Scaled rating = (rating - 1) / (5 - 1)\n",
    "### Scaled delivery time = (delivery time - 10) / (60 - 10)\n",
    "## After applying Min-Max scaling, all the features will be on the same scale, which will make it easier to compare them and use them to build a recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8d4973-239a-44a2-b183-715c542e03a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cb26bda-b837-4d26-bde1-70a0b0a2543b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "674c2ac2-97d2-49fe-8a3f-1dc2680e1581",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f6e6970-29d4-499e-84cb-244cba9072ef",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "925e9c2d-d878-434f-b5ad-9eac454683f6",
   "metadata": {},
   "source": [
    "# Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "## PCA is a technique used to reduce the dimensionality of a dataset by identifying the principal components of the data. In the case of a stock price prediction model, PCA can be used to reduce the dimensionality of the dataset by identifying the most important features.\n",
    "\n",
    "## To use PCA, we first need to standardize the data to ensure that all the features are on the same scale. Once we have standardized the data, we can use PCA to identify the principal components of the data. The principal components are the directions in which the data varies the most. we can then select the top k principal components to create a new set of k variables that are uncorrelated with each other.\n",
    "\n",
    "## The number of principal components to select depends on the amount of variance explained by each principal component. we can use the scree plot to determine the number of principal components to select. The scree plot shows the amount of variance explained by each principal component. we can select the top k principal components that explain the most variance.\n",
    "\n",
    "## After applying PCA, we will have a new set of variables that are uncorrelated with each other. These new variables can be used as features in a machine learning model to predict stock prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d780046b-e74e-446e-92b4-5e2164848ecd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5d6d3b7-3454-4475-a78a-940cbfcad2ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4761576-8e6a-415b-8614-ba179044eacb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0cccbe7-a098-4ae1-818d-515bc7948e24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88dc77e3-b56f-4f21-9ef1-43888e7c1d2e",
   "metadata": {},
   "source": [
    "# Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "## To perform Min-Max scaling on a dataset containing the values [1, 5, 10, 15, 20] to transform the values to a range of -1 to 1, we would first need to determine the minimum and maximum values of the dataset.\n",
    "\n",
    "### 1.The minimum value of the dataset is 1, and the maximum value of the dataset is 20.\n",
    "\n",
    "### 2.We can then use the following formula to scale the values of the dataset to a range of -1 to 1:\n",
    "\n",
    "#### scaled_value = ((value - min_value) / (max_value - min_value)) * 2 - 1\n",
    "\n",
    "### 3.Using this formula, we can scale the values of the dataset to a range of -1 to 1 as follows:\n",
    "\n",
    "### 4.After applying Min-Max scaling, all the values of the dataset will be within the range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ddfb99-40d6-4364-848b-417ebe1376b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "68ec88aa-3123-4805-afc0-df794bedac03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = [1,5,10,15,20]\n",
    "\n",
    "scaled_values = []\n",
    "\n",
    "min_value = 1\n",
    "max_value = 20\n",
    "data_range = max_value - min_value\n",
    "\n",
    "for value in dataset:\n",
    "    scaled_values.append(((value - min_value) / data_range) * 2 - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "13a11e21-2d57-4731-a29a-3b5716d9f4b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -0.5789473684210527, -0.052631578947368474, 0.4736842105263157, 1.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10664011-cb32-45ba-bd71-a387d40dc283",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84e47d21-3bcb-436c-a6ca-2c06e90f97c7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe12347e-1f31-4949-8545-c7f1a610737c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9acc213d-3805-4cd6-b025-2021bead5173",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5e8ebb1-dcf6-41b2-86bb-9fdc8917fb93",
   "metadata": {},
   "source": [
    "# Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "## To perform feature extraction using PCA on a dataset containing the features [height, weight, age, gender, blood pressure], we would first need to standardize the data to ensure that all the features are on the same scale. Once we have standardized the data, we can use PCA to identify the principal components of the data.\n",
    "\n",
    "## The number of principal components to retain depends on the amount of variance explained by each principal component. we can use the scree plot to determine the number of principal components to retain. The scree plot shows the amount of variance explained by each principal component. we can select the top k principal components that explain the most variance.\n",
    "\n",
    "## Without knowing the specifics of the dataset, it is difficult to say how many principal components to retain. However, as a general rule of thumb, we should retain enough principal components to explain at least 80% of the variance in the data.\n",
    "\n",
    "## In the case of a dataset containing the features [height, weight, age, gender, blood pressure], we might expect that height, weight, and blood pressure are the most important features. However, without knowing the specifics of the dataset, it is difficult to say for sure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
