{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b315a48-e962-48a9-b685-14039d972262",
   "metadata": {},
   "source": [
    "# ```Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1554d3a-21eb-4822-94db-4cef2568bc23",
   "metadata": {},
   "source": [
    "> # ``` Using Conditional Probability: ```\n",
    "## To solve this problem, we need to use Conditional Probability formula, which states that:\n",
    "\n",
    "- ### P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "- ### where A and B are events, and P(A|B) is the probability of event A given that event B has occurred.\n",
    "\n",
    "### In this case, we want to find the probability that an employee is a smoker given that he/she uses the health insurance plan, which is:\n",
    "\n",
    "- ### P(smoker | uses insurance) = P(smoker ∩ uses insurance) / P(uses insurance)\n",
    "\n",
    "## We know that 70% of employees use the insurance plan, so:\n",
    "\n",
    "- ### P(uses insurance) = 0.7\n",
    "## We also know that 40% of employees are smoker as well as plan holder, so:\n",
    "\n",
    "- ### P(smoker ∩ uses insurance) = 0.4\n",
    "\n",
    "### Putting it all together, we get:\n",
    "\n",
    "- ### P(smoker | uses insurance) = 0.4 / 0.7 = 0.57\n",
    "\n",
    "### Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.57 or 57%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ac76c9-26a8-4047-86e9-dad3c1a0e5c7",
   "metadata": {},
   "source": [
    "> # ``` Using Bayes' Theorem: ```\n",
    "## To solve this problem, we need to use Bayes' theorem, which states that:\n",
    "\n",
    "- ### P(A|B) = P(B|A) * P(A) / P(B)\n",
    "\n",
    "- ### where A and B are events, and P(A|B) is the probability of event A given that event B has occurred.\n",
    "\n",
    "### In this case, we want to find the probability that an employee is a smoker given that he/she uses the health insurance plan, which is:\n",
    "\n",
    "- ### P(smoker | uses insurance) = P(uses insurance | smoker) * P(smoker) / P(uses insurance)\n",
    "\n",
    "## We know that 70% of employees use the insurance plan, so:\n",
    "\n",
    "- ### P(uses insurance) = 0.7\n",
    "## We also know that 40% of employees who use the plan are smokers, so:\n",
    "\n",
    "- ### P(uses insurance | smoker) = 0.4\n",
    "\n",
    "## Finally, we know that 100% of smokers smoke, so:\n",
    "\n",
    "- ### P(smoker) = 1.0\n",
    "\n",
    "### Putting it all together, we get:\n",
    "\n",
    "- ### P(smoker | uses insurance) = 0.4 * 1.0 / 0.7 = 0.57\n",
    "\n",
    "### Therefore, the probability that an employee is a smoker given that he/she uses the health insurance plan is 0.57 or 57%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b0d8f1-6b54-467f-845a-5e59b2542ad3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d772b2cb-d3f9-4056-84fb-2ce58ec36c05",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4064368-c474-4048-9ebf-8aa68f4b93ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e61b4fa7-e0ee-445f-a6b7-cf6b553ec9ab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d31a5d7-1278-4af9-86b2-caac8083d484",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93044342-765d-41c8-a3f5-36b53b5dbe64",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ```Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?```\n",
    "### Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of Naive Bayes classification, but they differ in the type of data they are best suited for.\n",
    "\n",
    "## ```Bernoulli Naive Bayes``` is used when the input variables are binary, i.e., they take on values of 0 or 1. It is often used for text classification problems where the presence or absence of certain words in a document is used to determine its classification.\n",
    "\n",
    "## ```Multinomial Naive Bayes``` is used when the input variables represent counts or frequencies of events. It is often used for text classification problems where the number of occurrences of certain words in a document is used to determine its classification.\n",
    "\n",
    "### In summary, Bernoulli Naive Bayes is used for binary data while Multinomial Naive Bayes is used for count or frequency data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566a874b-148c-46e4-81ee-919b8770e4aa",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89dce7e1-e38c-41c7-bbd0-dd9199a91bd1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a283139-7c6b-454c-8a84-0212abc0e2e3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e465aec8-11d7-4aad-adb4-dbb3303b4031",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "defd6239-9317-4021-b725-1715d75abfb3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d9bd610-732d-4350-b1da-4914945f4d9a",
   "metadata": {},
   "source": [
    "# ```Q3. How does Bernoulli Naive Bayes handle missing values?```\n",
    "## ```Bernoulli Naive Bayes``` assumes that the input variables are binary, taking on values of 0 or 1. If a data point is missing a value for a particular input variable, it is typically treated as a 0 for that variable. This is because the absence of a value can be interpreted as a \"no\" or \"false\" response to the question of whether the feature is present.\n",
    "\n",
    "## However, if a significant number of data points have missing values for a particular input variable, the performance of the Bernoulli Naive Bayes classifier may be affected. In such cases, it may be more appropriate to impute the missing values using a suitable imputation technique or consider using a different classification algorithm that can handle missing data more effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe4fe36-9693-4a3a-b36f-654aca2f378d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a8b92f4-c088-4a2d-9882-357d54cffc9a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de704d28-96f4-4114-9258-6d36c01bfe10",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7a33510-256a-4e4f-91e3-cbcf5fdb5c30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d203a75-bd5c-4808-b74d-0b5db382940d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9eaa2190-8edc-4495-8197-4f890d3b7fd5",
   "metadata": {},
   "source": [
    "# ```Q4. Can Gaussian Naive Bayes be used for multi-class classification?```\n",
    "## Yes, ```Gaussian Naive Bayes``` can be used for multi-class classification. It is a probabilistic algorithm that assumes that the input variables are normally distributed, and it calculates the conditional probability of each class given the input variables.\n",
    "\n",
    "## For ```multi-class classification```, the algorithm calculates the conditional probability of each class given the input variables, and then selects the class with the highest probability as the predicted class. This approach is known as the \"one-vs-all\" approach, where each class is treated as a binary classification problem, and the algorithm is run for each class.\n",
    "\n",
    "## Alternatively, ```Gaussian Naive Bayes``` can be modified to directly model the joint probability of all classes and input variables, which is known as the \"one-vs-one\" approach. In this approach, the algorithm calculates the joint probability of each class and input variables, and then selects the class with the highest probability as the predicted class.\n",
    "\n",
    "### Overall, Gaussian Naive Bayes is a versatile algorithm that can be used for both binary and multi-class classification problems, although its assumptions about the input variables may not always hold in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7808d521-1295-4211-9d8e-68526664ca54",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a7cbcbc-c1e5-463d-9025-65a9a74e066e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fc871c6-64d2-4171-aa7d-afe45dfd730d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "26b6fbb9-22cf-4450-9c38-63c970bca71b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d8ffd75-6cc1-4e6e-ba9d-1d2f7bec5dc1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "598f10f0-1121-4d41-a266-5a12f08ed456",
   "metadata": {},
   "source": [
    "> # ```Q5.```\n",
    "## ```Implementation:```\n",
    "\n",
    "## Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the dataset. You should use the default hyperparameters for each classifier.\n",
    "\n",
    "\n",
    "## ```Results:```\n",
    "\n",
    "## Report the following performance metrics for each classifier:\n",
    "- ### Accuracy\n",
    "- ### Precision\n",
    "- ### Recall\n",
    "- ### F1 score\n",
    "\n",
    "## ```Discussion:```\n",
    "\n",
    "### Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is the case? Are there any limitations of Naive Bayes that you observed?\n",
    "\n",
    "## ```Conclusion:```\n",
    "\n",
    "## Summarise your findings and provide some suggestions for future work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8b689c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de27d486",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spams = pd.read_csv('spambase.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07aedb77",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spams.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "57833572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4601 entries, 0 to 4600\n",
      "Data columns (total 58 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   word_freq_make              4601 non-null   float64\n",
      " 1   word_freq_address           4601 non-null   float64\n",
      " 2   word_freq_all               4601 non-null   float64\n",
      " 3   word_freq_3d                4601 non-null   float64\n",
      " 4   word_freq_our               4601 non-null   float64\n",
      " 5   word_freq_over              4601 non-null   float64\n",
      " 6   word_freq_remove            4601 non-null   float64\n",
      " 7   word_freq_internet          4601 non-null   float64\n",
      " 8   word_freq_order             4601 non-null   float64\n",
      " 9   word_freq_mail              4601 non-null   float64\n",
      " 10  word_freq_receive           4601 non-null   float64\n",
      " 11  word_freq_will              4601 non-null   float64\n",
      " 12  word_freq_people            4601 non-null   float64\n",
      " 13  word_freq_report            4601 non-null   float64\n",
      " 14  word_freq_addresses         4601 non-null   float64\n",
      " 15  word_freq_free              4601 non-null   float64\n",
      " 16  word_freq_business          4601 non-null   float64\n",
      " 17  word_freq_email             4601 non-null   float64\n",
      " 18  word_freq_you               4601 non-null   float64\n",
      " 19  word_freq_credit            4601 non-null   float64\n",
      " 20  word_freq_your              4601 non-null   float64\n",
      " 21  word_freq_font              4601 non-null   float64\n",
      " 22  word_freq_000               4601 non-null   float64\n",
      " 23  word_freq_money             4601 non-null   float64\n",
      " 24  word_freq_hp                4601 non-null   float64\n",
      " 25  word_freq_hpl               4601 non-null   float64\n",
      " 26  word_freq_george            4601 non-null   float64\n",
      " 27  word_freq_650               4601 non-null   float64\n",
      " 28  word_freq_lab               4601 non-null   float64\n",
      " 29  word_freq_labs              4601 non-null   float64\n",
      " 30  word_freq_telnet            4601 non-null   float64\n",
      " 31  word_freq_857               4601 non-null   float64\n",
      " 32  word_freq_data              4601 non-null   float64\n",
      " 33  word_freq_415               4601 non-null   float64\n",
      " 34  word_freq_85                4601 non-null   float64\n",
      " 35  word_freq_technology        4601 non-null   float64\n",
      " 36  word_freq_1999              4601 non-null   float64\n",
      " 37  word_freq_parts             4601 non-null   float64\n",
      " 38  word_freq_pm                4601 non-null   float64\n",
      " 39  word_freq_direct            4601 non-null   float64\n",
      " 40  word_freq_cs                4601 non-null   float64\n",
      " 41  word_freq_meeting           4601 non-null   float64\n",
      " 42  word_freq_original          4601 non-null   float64\n",
      " 43  word_freq_project           4601 non-null   float64\n",
      " 44  word_freq_re                4601 non-null   float64\n",
      " 45  word_freq_edu               4601 non-null   float64\n",
      " 46  word_freq_table             4601 non-null   float64\n",
      " 47  word_freq_conference        4601 non-null   float64\n",
      " 48  char_freq_;                 4601 non-null   float64\n",
      " 49  char_freq_(                 4601 non-null   float64\n",
      " 50  char_freq_[                 4601 non-null   float64\n",
      " 51  char_freq_!                 4601 non-null   float64\n",
      " 52  char_freq_$                 4601 non-null   float64\n",
      " 53  char_freq_#                 4601 non-null   float64\n",
      " 54  capital_run_length_average  4601 non-null   float64\n",
      " 55  capital_run_length_longest  4601 non-null   int64  \n",
      " 56  capital_run_length_total    4601 non-null   int64  \n",
      " 57  spam                        4601 non-null   int64  \n",
      "dtypes: float64(55), int64(3)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "spams.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e11b4683",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000  ...  4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413  ...     0.038575     0.139030   \n",
       "std           0.278616        0.644755  ...     0.243471     0.270355   \n",
       "min           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "25%           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "50%           0.000000        0.000000  ...     0.000000     0.065000   \n",
       "75%           0.000000        0.160000  ...     0.000000     0.188000   \n",
       "max           5.260000       18.180000  ...     4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total         spam  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spams.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db6b4f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spams.isnull().any().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc240de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4601, 58)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spams.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "582f9c8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = spams.drop('spam', axis=1)\n",
    "y = spams.spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08216c1e",
   "metadata": {},
   "source": [
    "> # Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3eb507cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.05,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7df180c",
   "metadata": {},
   "source": [
    "> # Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4a7a838f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b989302d",
   "metadata": {},
   "source": [
    "> # Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "09e4b70f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.25 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "model = BernoulliNB()\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "scores = cross_val_score(model,X_train,y_train, cv=kfold)\n",
    "print(f\"Accuracy: {round(scores.mean(),4)*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aeeb91-e67b-4ddd-afb7-1fa0d6f0728b",
   "metadata": {},
   "source": [
    "> ## Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c0b69d78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[116  11]\n",
      " [ 11  93]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.91      0.91       127\n",
      "           1       0.89      0.89      0.89       104\n",
      "\n",
      "    accuracy                           0.90       231\n",
      "   macro avg       0.90      0.90      0.90       231\n",
      "weighted avg       0.90      0.90      0.90       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63097daf",
   "metadata": {},
   "source": [
    "> # Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "67c267be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.14 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "model = GaussianNB()\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "scores = cross_val_score(model,X_train,y_train, cv=kfold)\n",
    "print(f\"Accuracy: {round(scores.mean(),4)*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8581ac-5082-443d-b7b6-d517ab4768f1",
   "metadata": {},
   "source": [
    "> ## Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88ed0dd9-7aac-48e3-9686-08fbbb2f48e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 93  34]\n",
      " [  2 102]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.73      0.84       127\n",
      "           1       0.75      0.98      0.85       104\n",
      "\n",
      "    accuracy                           0.84       231\n",
      "   macro avg       0.86      0.86      0.84       231\n",
      "weighted avg       0.88      0.84      0.84       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365879c",
   "metadata": {},
   "source": [
    "> # Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ba12e5bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.05,random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1636f8",
   "metadata": {},
   "source": [
    "> ## MinMax Scaling for Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b40aa658",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fbf82358-e761-4735-b7a5-de15b5830ec2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.47 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "model = MultinomialNB()\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=1)\n",
    "scores = cross_val_score(model,X_train,y_train, cv=kfold)\n",
    "print(f\"Accuracy: {round(scores.mean(),4)*100} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18036ab2-9089-4a7e-a92a-eae59f97522a",
   "metadata": {},
   "source": [
    "> ## Confusion Matrix & Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9d7aaf49-0277-4ffc-9679-cec44a1f5f0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[122   5]\n",
      " [ 19  85]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91       127\n",
      "           1       0.94      0.82      0.88       104\n",
      "\n",
      "    accuracy                           0.90       231\n",
      "   macro avg       0.90      0.89      0.89       231\n",
      "weighted avg       0.90      0.90      0.90       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d2e484",
   "metadata": {},
   "source": [
    "> # Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89fa62",
   "metadata": {},
   "source": [
    "## After training our model using Bernoulli, Gaussian and Multinomial naive bayes algorithm at same test size and random state with  KFold of 10, We have following results:\n",
    "* ### Bernoulli Accuracy: 90.25 %\n",
    "* ### Gaussian Accuracy: 81.14 %\n",
    "* ### Multinomial Accuracy: 88.47 %\n",
    "\n",
    "## Therefore, The Bernaoulli naive bayes perform most accurately than other two, As we know the Bernoulli is specialized for binary class. And our targer variable is also a binary class variable.\n",
    "\n",
    "\n",
    "## Yes, there are some limitations of Naive Bayes:\n",
    "\n",
    " * ### 1. ```Assumption of independence```: Naive Bayes assumes that all features are independent of each other, which may not be true in all cases. If the features are correlated, then Naive Bayes may not perform well.\n",
    "\n",
    "* ### 2. ```Limited expressiveness```: Naive Bayes can only represent linear decision boundaries. It may not be able to capture more complex decision boundaries that are necessary in some datasets.\n",
    "\n",
    "* ### 3. ```Sensitivity to irrelevant features```: Naive Bayes treats all features equally, even if some features may not be relevant for classification. This can lead to reduced performance if irrelevant features are included in the model.\n",
    "\n",
    "* ### 4. ```Lack of probability estimates```: Naive Bayes does not provide accurate probability estimates for the predicted class, and may not be appropriate if probabilities are important.\n",
    "\n",
    "* ### 5. ```Limited data```: Naive Bayes may not perform well if there is limited training data available, as it relies heavily on the training data to estimate the parameters of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57429caa",
   "metadata": {},
   "source": [
    "> # Summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
