{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "807c90b1-cd49-4ef3-818b-b91176163ece",
   "metadata": {},
   "source": [
    "# Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "## Missing values in a dataset are the values that are not present in the dataset. It is essential to handle missing values because they can cause problems when using the dataset for any machine learning algorithm. Even if we set apart the algorithm perspective, missing values are really undesirable. They hinder with data analysis and data visualization.\n",
    "\n",
    "## Some of the machine learning algorithms that are not affected by missing values are k-NN algorithm, Naive Bayes, Histogram based Gradient-boosting Classifier / Regressor, and SVM algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf15136-3e6d-4e70-9cf5-69b98c8c3824",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "766a5fbb-4b5c-4065-8499-4968cac88775",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "449c9f1f-2e8a-4af8-9da3-338365378cf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54c3e55b-79f6-44ca-ab72-1f9f9bfb4fb2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ed0b893-eb44-4134-aa5a-b1b648372b63",
   "metadata": {},
   "source": [
    "# Q2: List down techniques used to handle missing data. Give an example of each with python code.\n",
    "## There are several techniques to handle missing data. Here are some of the most common techniques with an example of each using Python code:\n",
    "\n",
    "## 1.Deletion: In this technique, we delete the rows or columns that contain missing values. This technique is used when the number of missing values is very small. Here is an example of how to delete rows with missing values using Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d05d50-c085-40eb-94ac-a01919b283a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc0d3ab-7672-4e04-b523-169c0e5b42f7",
   "metadata": {},
   "source": [
    "## 2. Mean/Mode/Median Imputation: In this technique, we replace the missing values with the mean/mode/median of the column. This technique is used when the number of missing values is small. Here is an example of how to replace missing values with the mean of the column using Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2bc14f-e7f4-4872-b882-694805d96bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data.csv')\n",
    "df.fillna(df.mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aac210-c68f-40f6-b6de-54e749a17331",
   "metadata": {},
   "source": [
    "## 3.K-Nearest Neighbors Imputation: In this technique, we replace the missing values with the values of the k-nearest neighbors. This technique is used when the number of missing values is moderate. Here is an example of how to replace missing values with the values of the k-nearest neighbors using Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca01c51-6ce4-4bfc-a0fc-eb36e6869994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "df = pd.read_csv('data.csv')\n",
    "imputer = KNNImputer(n_neighbors=2)\n",
    "df = pd.DataFrame(imputer.fit_transform(df), columns = df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480c90bd-fff1-445d-9db3-585439bc9ae7",
   "metadata": {},
   "source": [
    "## 4.Regression Imputation: In this technique, we use regression to predict the missing values. This technique is used when the number of missing values is moderate. Here is an example of how to replace missing values using regression using Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b346dd46-3665-4856-8ccf-b9753bf60c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "df = pd.read_csv('data.csv')\n",
    "df_train = df.dropna()\n",
    "df_test = df[df.isna().any(axis=1)]\n",
    "X_train = df_train.drop('target', axis=1)\n",
    "y_train = df_train['target']\n",
    "X_test = df_test.drop('target', axis=1)\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "df_test['target'] = y_pred\n",
    "df = pd.concat([df_train, df_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f1e108-b522-4050-9f1d-bbd194233f16",
   "metadata": {},
   "source": [
    "## 5.Multiple Imputation: In this technique, we create multiple imputations of the missing values and then combine them to get the final result. This technique is used when the number of missing values is large. Here is an example of how to use multiple imputation using Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58efe23c-8bc8-4a34-9a05-a2e704dd78fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "df = pd.read_csv('data.csv')\n",
    "imputer = IterativeImputer()\n",
    "df = pd.DataFrame(imputer.fit_transform(df), columns = df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d7b90e-78f6-4fdf-af59-c9d10bef8d9b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "02c158b9-631e-4ea0-be64-85d006099522",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "224c2b38-d8a8-4b0b-9b18-3f9a6d4c9558",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "57b58338-d8c4-471d-bd1b-5b50cfb2fb7e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71d33202-3c7b-4033-a03b-e80a5c257de9",
   "metadata": {},
   "source": [
    "# Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "## Imbalanced data is a situation where the number of observations in one class is significantly higher or lower than the number of observations in another class. For example, if we have a binary classification problem where 90% of the observations belong to class A and only 10% of the observations belong to class B, then we have an imbalanced dataset.\n",
    "\n",
    "## If imbalanced data is not handled, then the machine learning algorithm will be biased towards the majority class. This means that the algorithm will perform well on the majority class but will perform poorly on the minority class. This is because the algorithm will learn to predict the majority class more often than the minority class. This can be a problem in many real-world scenarios where the minority class is of more interest than the majority class.\n",
    "\n",
    "## To handle imbalanced data, we can use techniques such as undersampling, oversampling, SMOTE, and cost-sensitive learning. These techniques can help us balance the dataset and improve the performance of the machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61278b4b-e0c7-468b-b90f-4e8a2fcaba5d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c6cfbc1-1072-477c-b429-7c4bc4bd590f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96928993-1ceb-40c1-bb21-e0cb4d317723",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37608bcc-019e-4499-9a85-df4cc7debfc9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aeab240-972a-4b88-98a8-89c4b405bcf5",
   "metadata": {},
   "source": [
    "# Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down-sampling are required.\n",
    "## Up-sampling and down-sampling are techniques used to balance an imbalanced dataset.\n",
    "\n",
    "## 1.Up-sampling is a technique where we increase the number of observations in the minority class by randomly replicating them. This technique is used when the number of observations in the minority class is very small. For example, if we have a binary classification problem where 90% of the observations belong to class A and only 10% of the observations belong to class B, then we can use up-sampling to increase the number of observations in class B.\n",
    "\n",
    "## 2.Down-sampling is a technique where we decrease the number of observations in the majority class by randomly removing them. This technique is used when the number of observations in the majority class is very large. For example, if we have a binary classification problem where 90% of the observations belong to class A and only 10% of the observations belong to class B, then we can use down-sampling to decrease the number of observations in class A.\n",
    "\n",
    "## Here is an example of when up-sampling and down-sampling are required:\n",
    "\n",
    "### Suppose we have a dataset of 1000 observations where 900 observations belong to class A and 100 observations belong to class B. This is an imbalanced dataset. If we use this dataset to train a machine learning algorithm, then the algorithm will be biased towards class A. This means that the algorithm will perform well on class A but will perform poorly on class B.\n",
    "\n",
    "## To handle this situation, we can use up-sampling or down-sampling. If we use up-sampling, then we will randomly replicate the 100 observations in class B to create a new dataset of 1000 observations where 500 observations belong to class A and 500 observations belong to class B. If we use down-sampling, then we will randomly remove 800 observations from class A to create a new dataset of 200 observations where 100 observations belong to class A and 100 observations belong to class B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447df8d7-3bcb-44bf-8b6a-e336718fa4e2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cddfaaa-4423-4f92-8e1a-3da676695b1d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47a250fb-3513-4134-856d-7bcc57be415d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30e93993-10b8-4368-84bd-6c9db6eceaab",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d75f7776-507e-4138-ab84-490a7db6352f",
   "metadata": {},
   "source": [
    "# Q5: What is data Augmentation? Explain SMOTE.\n",
    "## Data augmentation is a technique used to increase the size of a dataset by creating new data based on the existing data. This technique is used to improve the performance of machine learning algorithms by providing more data for training.\n",
    "\n",
    "## SMOTE (Synthetic Minority Over-sampling Technique) is a data augmentation algorithm that creates synthetic data points based on the original data points. SMOTE is used to handle imbalanced datasets where the number of observations in one class is significantly higher or lower than the number of observations in another class. SMOTE creates synthetic data points for the minority class by interpolating between the existing data points. This technique is used to balance the dataset and improve the performance of the machine learning algorithm.\n",
    "\n",
    "### For example, suppose we have a binary classification problem where 90% of the observations belong to class A and only 10% of the observations belong to class B. This is an imbalanced dataset. If we use this dataset to train a machine learning algorithm, then the algorithm will be biased towards class A. This means that the algorithm will perform well on class A but will perform poorly on class B.\n",
    "\n",
    "## To handle this situation, we can use SMOTE to create synthetic data points for class B. SMOTE will create new data points for class B by interpolating between the existing data points. This will balance the dataset and improve the performance of the machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c8f9ee-e85a-43a5-bb6f-abff5badee9e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5c63651-66ce-40b4-b945-df5181b2d4ad",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88c5b708-e60b-45d4-b1b8-397acc7e6132",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d73becb-30fe-49d3-910c-49d1ac531ef5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab95c026-a49c-42d1-93ba-1218b0ae284e",
   "metadata": {},
   "source": [
    "# Q6: What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "## Outliers are data points that are significantly different from other data points in a dataset. Outliers can be caused by measurement errors, data entry errors, or other factors. Outliers can have a significant impact on the performance of machine learning algorithms.\n",
    "\n",
    "## It is essential to handle outliers because they can cause the machine learning algorithm to learn from incorrect data. Outliers can also cause the machine learning algorithm to overfit the data. Overfitting occurs when the machine learning algorithm learns the noise in the data instead of the underlying pattern. This can cause the machine learning algorithm to perform poorly on new data.\n",
    "\n",
    "## To handle outliers, we can use techniques such as removing outliers, clipping, and imputation. Removing outliers involves removing the data points that are significantly different from other data points in the dataset. Clipping involves setting the values of the outliers to a fixed value. Imputation involves replacing the outliers with a value that is more representative of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7edbfed-dfcc-42b9-847e-01a537f320d1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33c532d1-f653-42f3-acf7-23e60e3ad61a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2e97ed6-d834-4ccb-afa1-1641fff0d583",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "935e5d0f-8020-4083-a41d-a37ddfbea470",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88cb7b81-92bd-4eb4-a593-e64c47195b51",
   "metadata": {},
   "source": [
    "# Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "## There are several techniques that can be used to handle missing data in a dataset. Here are some of the most common techniques:\n",
    "\n",
    "## 1.Deletion: This technique involves deleting the rows or columns that contain missing data. This technique is used when the amount of missing data is small.\n",
    "\n",
    "## 2.Imputation: This technique involves replacing the missing data with a value that is more representative of the data. There are several methods for imputing missing data, including mean imputation, median imputation, and regression imputation.\n",
    "\n",
    "## 3.Prediction: This technique involves using machine learning algorithms to predict the missing data. This technique is used when the amount of missing data is large.\n",
    "\n",
    "## 4.Interpolation: This technique involves estimating the missing data based on the values of the neighboring data points. This technique is used when the data is time-series data.\n",
    "\n",
    "### The choice of technique depends on the amount of missing data, the type of data, and the analysis that is being performed. It is essential to handle missing data because missing data can cause the machine learning algorithm to learn from incorrect data. Missing data can also cause the machine learning algorithm to overfit the data. Overfitting occurs when the machine learning algorithm learns the noise in the data instead of the underlying pattern. This can cause the machine learning algorithm to perform poorly on new data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c34d0-bd0d-4d25-9f51-491d9f4f358f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25cbb3c3-d7a1-4846-8615-9ff0c872e8a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c14d90f3-875b-4e20-860b-c61eebaeb0a9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49fb4f1e-c702-4e6c-af82-55931636066c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2042ca04-3a16-45df-a3aa-e2a6c9ef3617",
   "metadata": {},
   "source": [
    "# Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "## There are several strategies that can be used to determine if the missing data is missing at random or if there is a pattern to the missing data. Here are some of the most common strategies:\n",
    "\n",
    "## 1.Visual inspection: This involves plotting the data to see if there is a pattern to the missing data. For example, if the missing data is clustered around a particular value, then there may be a pattern to the missing data.\n",
    "\n",
    "## 2.Statistical tests: This involves using statistical tests to determine if the missing data is missing at random or if there is a pattern to the missing data. For example, the Littleâ€™s MCAR test can be used to determine if the missing data is missing completely at random.\n",
    "\n",
    "## 3.Machine learning algorithms: This involves using machine learning algorithms to predict the missing data. If the machine learning algorithm can predict the missing data accurately, then the missing data is likely missing at random.\n",
    "\n",
    "## The choice of strategy depends on the amount of missing data, the type of data, and the analysis that is being performed. It is essential to determine if the missing data is missing at random or if there is a pattern to the missing data because this can affect the analysis that is being performed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc9cee9-49eb-4826-a502-c1b869c84485",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d6f832f-4800-4642-9d90-548a4e7d03a8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d8489b0-3363-4537-92bf-60270c571b48",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d39e8cf9-15bd-4668-ae84-7e3f5b0ed316",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4597302d-048f-47cf-8294-888499d2bfe0",
   "metadata": {},
   "source": [
    "# Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "## When working with an imbalanced dataset, it is essential to evaluate the performance of the machine learning model carefully. Here are some strategies that can be used to evaluate the performance of the machine learning model on an imbalanced dataset:\n",
    "\n",
    "## 1.Confusion matrix: This is a table that shows the number of true positives, false positives, true negatives, and false negatives. The confusion matrix can be used to calculate metrics such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "## 2.ROC curve: This is a curve that shows the true positive rate (sensitivity) against the false positive rate (1-specificity) at different classification thresholds. The ROC curve can be used to calculate the area under the curve (AUC), which is a measure of the performance of the machine learning model.\n",
    "\n",
    "## 3.Precision-Recall curve: This is a curve that shows the precision against the recall at different classification thresholds. The precision-recall curve can be used to calculate the area under the curve (AUC), which is a measure of the performance of the machine learning model.\n",
    "\n",
    "## 4.Cost-sensitive learning: This involves assigning different costs to different types of errors. For example, a false negative may be more costly than a false positive. Cost-sensitive learning can be used to improve the performance of the machine learning model on the minority class.\n",
    "\n",
    "### The choice of strategy depends on the amount of missing data, the type of data, and the analysis that is being performed. It is essential to evaluate the performance of the machine learning model carefully when working with an imbalanced dataset because the machine learning model may be biased towards the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08820f0-c495-4b14-a4e6-d47e88eb53a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67840585-570d-41ae-8324-4cdf2451cb01",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9ea5ebd-906e-4864-9f82-796fd0aa1ee6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "530e8b6e-1f1b-47bd-aebc-915a40526e64",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4289eef-0fed-4fa9-8370-82317f45e5b4",
   "metadata": {},
   "source": [
    "# Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "## There are several methods that can be used to balance an unbalanced dataset and down-sample the majority class. Here are some of the most common methods:\n",
    "\n",
    "## 1.Undersampling: This involves reducing the size of the majority class to balance the dataset. This can be done by randomly selecting a subset of the majority class.\n",
    "\n",
    "## 2.Oversampling: This involves increasing the size of the minority class to balance the dataset. This can be done by duplicating the minority class or by generating synthetic samples using techniques such as SMOTE.\n",
    "\n",
    "## 3.Cost-sensitive learning: This involves assigning different costs to different types of errors. For example, a false negative may be more costly than a false positive. Cost-sensitive learning can be used to improve the performance of the machine learning model on the minority class.\n",
    "\n",
    "## 4.Ensemble learning: This involves combining multiple machine learning models to improve the performance of the machine learning model on the minority class.\n",
    "\n",
    "### The choice of method depends on the amount of data, the type of data, and the analysis that is being performed. It is essential to balance the dataset when working with an unbalanced dataset because the machine learning model may be biased towards the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b05ae12-b3ae-4135-95d1-b9a3815abc23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65747944-ae1b-4211-b242-fea608471154",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4cc913db-9489-4390-ae13-7de3f5e6184c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "96b24217-d679-44ad-84b2-27eefb20d8eb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09febf5a-c2b8-4c16-8418-0c5ca36be529",
   "metadata": {},
   "source": [
    "# Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "## When working with a dataset that has a low percentage of occurrences, there are several methods that can be used to balance the dataset and up-sample the minority class. Here are some of the most common methods:\n",
    "\n",
    "## 1.Oversampling: This involves increasing the size of the minority class to balance the dataset. This can be done by duplicating the minority class or by generating synthetic samples using techniques such as SMOTE.\n",
    "\n",
    "## 2.Cost-sensitive learning: This involves assigning different costs to different types of errors. For example, a false negative may be more costly than a false positive. Cost-sensitive learning can be used to improve the performance of the machine learning model on the minority class.\n",
    "\n",
    "## 3.Ensemble learning: This involves combining multiple machine learning models to improve the performance of the machine learning model on the minority class.\n",
    "\n",
    "## 4.Resampling: This involves removing samples from the majority class (under-sampling) and/or adding more examples from the minority class (over-sampling) to balance the dataset.\n",
    "\n",
    "### The choice of method depends on the amount of data, the type of data, and the analysis that is being performed. It is essential to balance the dataset when working with an unbalanced dataset because the machine learning model may be biased towards the majority class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
